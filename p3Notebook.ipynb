{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBFvvcU05V-X"
   },
   "outputs": [],
   "source": [
    "### Install libraries ###\n",
    "\n",
    "#!pip install git+https://github.com/HumanCompatibleAI/overcooked_ai.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBFvvcU05V-X"
   },
   "outputs": [],
   "source": [
    "### Install libraries ###\n",
    "\n",
    "#!pip install git+https://github.com/HumanCompatibleAI/overcooked_ai.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ScQhFpxT5ZkL"
   },
   "outputs": [],
   "source": [
    "### Imports ###\n",
    "\n",
    "from overcooked_ai_py.mdp.overcooked_mdp import OvercookedGridworld\n",
    "from overcooked_ai_py.mdp.overcooked_env import OvercookedEnv\n",
    "from overcooked_ai_py.agents.benchmarking import AgentEvaluator\n",
    "from overcooked_ai_py.visualization.state_visualizer import StateVisualizer\n",
    "from overcooked_ai_py.agents.agent import NNPolicy, AgentFromPolicy, AgentPair\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from IPython.display import display, Image as IPImage\n",
    "\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "# from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System set up\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "\n",
    "def plot_soup(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(n_soups, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "        \n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_obs, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_obs, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, n_obs, n_actions):\n",
    "        self.batch_size = 128\n",
    "        self.memory = deque([], maxlen=100000)\n",
    "        self.epsilon = 0.95\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        self.learning_rate = 0.0001\n",
    "        self.tau = 0.005\n",
    "        self.gamma = 0.99\n",
    "        self.n_obs = n_obs\n",
    "        self.n_actions = n_actions\n",
    "        self.device = \"cpu\"\n",
    "        self.policy = DQN(n_obs, n_actions).to(self.device)\n",
    "        self.target = DQN(n_obs, n_actions).to(self.device)\n",
    "        self.target.load_state_dict(self.policy.state_dict())\n",
    "        self.target.eval()\n",
    "        self.optimizer = optim.AdamW(self.policy.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def memorize(self, obs, action, reward, next_obs, done):\n",
    "        obs = torch.tensor(obs, device=self.device, dtype=torch.float32)\n",
    "        action = torch.tensor(action, device=self.device, dtype=torch.long)\n",
    "        reward = torch.tensor(reward, device=self.device, dtype=torch.float32)\n",
    "        next_obs = torch.tensor(next_obs, device=self.device, dtype=torch.float32)\n",
    "        done = torch.tensor(done, device=self.device, dtype=torch.float32)\n",
    "        self.memory.append((obs, action, reward, next_obs, done))\n",
    "\n",
    "    def epsilon_greedy(self, obs):\n",
    "       # if self.epsilon > self.epsilon_min:\n",
    "        #    self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "        if random.random() >= self.epsilon:\n",
    "            obs = torch.tensor(obs, device=self.device, dtype=torch.float32)\n",
    "            q = self.policy(obs)\n",
    "            return torch.argmax(q).item()\n",
    "        else:\n",
    "            return random.randrange(self.n_actions)\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "            \n",
    "        transitions = random.sample(self.memory, self.batch_size)\n",
    "        obs, action, reward, next_obs, done = zip(*transitions)\n",
    "\n",
    "        obs = torch.stack(obs)\n",
    "        action = torch.stack(action)\n",
    "        next_obs = torch.stack(next_obs)\n",
    "        reward = torch.stack(reward)\n",
    "        done = torch.stack(done)\n",
    "        \n",
    "        predicted_Q = self.policy(obs).gather(1, action.unsqueeze(1))\n",
    "        # Compute the expected Q values\n",
    "        next_state_values = self.target(next_obs).max(1)[0]\n",
    "        target_Q = (next_state_values * self.gamma) * (1-done) + reward\n",
    "\n",
    "        # Compute SmoothL1Loss\n",
    "        loss = nn.SmoothL1Loss()(predicted_Q, target_Q.detach().unsqueeze(1))\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(self.policy.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        #soft update\n",
    "        for target_param, policy_param in zip(self.target.parameters(), self.policy.parameters()):\n",
    "            target_param.data.copy_(self.tau * policy_param.data + (1.0 -self.tau) * target_param.data)\n",
    "   #     target_nn_state_dict = self.target.state_dict()\n",
    "   #     policy_nn_state_dict = self.policy.state_dict()\n",
    "   #     for key in policy_nn_state_dict:\n",
    "   #         target_nn_state_dict[key] = policy_nn_state_dict[key]*self.tau + target_nn_state_dict[key]*(1-self.tau)\n",
    "   #     self.target.load_state_dict(target_nn_state_dict)          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Environment setup ###\n",
    "#random.seed(2023)\n",
    "device = \"cpu\"\n",
    "\n",
    "# Swap between the 5 layouts here:\n",
    "layout = \"cramped_room\"\n",
    "# layout = \"asymmetric_advantages\"\n",
    "# layout = \"coordination_ring\"\n",
    "# layout = \"forced_coordination\"\n",
    "# layout = \"counter_circuit_o_1order\"\n",
    "\n",
    "# Reward shaping is disabled by default.  This data structure may be used for\n",
    "# reward shaping.  You can, of course, do your own reward shaping in lieu of, or\n",
    "# in addition to, using this structure.\n",
    "reward_shaping = {\n",
    "    \"PLACEMENT_IN_POT_REW\": 3,\n",
    "    \"DISH_PICKUP_REWARD\": 3,\n",
    "    \"SOUP_PICKUP_REWARD\": 5\n",
    "}\n",
    "\n",
    "# Length of Episodes.  Do not modify for your submission!\n",
    "# Modification will result in a grading penalty!\n",
    "horizon = 400\n",
    "\n",
    "# Build the environment.  Do not modify!\n",
    "mdp = OvercookedGridworld.from_layout_name(layout, rew_shaping_params=reward_shaping)\n",
    "base_env = OvercookedEnv.from_mdp(mdp, horizon=horizon, info_level=0)\n",
    "env = gym.make(\"Overcooked-v0\", base_env=base_env,\n",
    "               featurize_fn=base_env.featurize_state_mdp)\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "obs = env.reset()\n",
    "obs0 = obs[\"both_agent_obs\"][0]\n",
    "obs1 = obs[\"both_agent_obs\"][1]\n",
    "n_obs = len(obs0)\n",
    "\n",
    "agent0 = DQNAgent(n_obs, n_actions)\n",
    "agent1 = DQNAgent(n_obs, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 3], [0, 5], [3, 0]]\n",
      "Ep 1 number of soups made: 0\n",
      "[[3, 0], [3, 0], [0, 3], [0, 3], [0, 5]]\n",
      "Ep 2 number of soups made: 0\n",
      "[[3, 0]]\n",
      "Ep 3 number of soups made: 0\n",
      "[[3, 0], [5, 0], [0, 3]]\n",
      "Ep 4 number of soups made: 0\n",
      "[[3, 0], [5, 0], [0, 3]]\n",
      "Ep 5 number of soups made: 0\n",
      "[[3, 0]]\n",
      "Ep 6 number of soups made: 0\n",
      "[[3, 0], [0, 3], [0, 5]]\n",
      "Ep 7 number of soups made: 0\n",
      "[[3, 0]]\n",
      "Ep 8 number of soups made: 0\n",
      "[]\n",
      "Ep 9 number of soups made: 0\n",
      "[[0, 3]]\n",
      "Ep 10 number of soups made: 0\n",
      "[]\n",
      "Ep 11 number of soups made: 0\n",
      "[[3, 0]]\n",
      "Ep 12 number of soups made: 0\n",
      "[[0, 3]]\n",
      "Ep 13 number of soups made: 0\n",
      "[[3, 0], [0, 3]]\n",
      "Ep 14 number of soups made: 0\n",
      "[[3, 0], [5, 0]]\n",
      "Ep 15 number of soups made: 0\n",
      "[[3, 0]]\n",
      "Ep 16 number of soups made: 0\n",
      "[[3, 0], [3, 0], [0, 5]]\n",
      "Ep 17 number of soups made: 0\n",
      "[[3, 0], [0, 3], [5, 0], [0, 3], [0, 3], [3, 0]]\n",
      "Ep 18 number of soups made: 0\n",
      "[[0, 3], [3, 0]]\n",
      "Ep 19 number of soups made: 0\n",
      "[[3, 0], [3, 0]]\n",
      "Ep 20 number of soups made: 0\n",
      "[[0, 3], [0, 3], [0, 5], [3, 0], [3, 0], [0, 3], [0, 5]]\n",
      "Ep 21 number of soups made: 0\n",
      "[[0, 3]]\n",
      "Ep 22 number of soups made: 0\n",
      "[[0, 3]]\n",
      "Ep 23 number of soups made: 0\n",
      "[[3, 0], [0, 3], [5, 0]]\n",
      "Ep 24 number of soups made: 0\n",
      "[[3, 0]]\n",
      "Ep 25 number of soups made: 0\n",
      "[[0, 3]]\n",
      "Ep 26 number of soups made: 0\n",
      "[[0, 3], [0, 3]]\n",
      "Ep 27 number of soups made: 0\n",
      "[]\n",
      "Ep 28 number of soups made: 0\n",
      "[[0, 3], [3, 0], [5, 0], [0, 3]]\n",
      "Ep 29 number of soups made: 0\n",
      "[[3, 0], [0, 3], [5, 0], [3, 0], [0, 3]]\n",
      "Ep 30 number of soups made: 0\n",
      "[[0, 3], [3, 0]]\n",
      "Ep 31 number of soups made: 0\n",
      "[[0, 3]]\n",
      "Ep 32 number of soups made: 0\n",
      "[[0, 3], [0, 3], [0, 5], [3, 0], [0, 5]]\n",
      "Ep 33 number of soups made: 0\n",
      "[[0, 3], [0, 3], [0, 3], [0, 5]]\n",
      "Ep 34 number of soups made: 0\n",
      "[[3, 0], [3, 0], [5, 0], [0, 3]]\n",
      "Ep 35 number of soups made: 0\n",
      "[[0, 3]]\n",
      "Ep 36 number of soups made: 0\n",
      "[[0, 3], [0, 3], [5, 0]]\n",
      "Ep 37 number of soups made: 0\n",
      "[[0, 3], [0, 3], [0, 5]]\n",
      "Ep 38 number of soups made: 0\n",
      "[[3, 0], [3, 0]]\n",
      "Ep 39 number of soups made: 0\n",
      "[[0, 3], [5, 0]]\n",
      "Ep 40 number of soups made: 0\n",
      "[[0, 3], [0, 3], [3, 0]]\n",
      "Ep 41 number of soups made: 0\n",
      "[[0, 3]]\n",
      "Ep 42 number of soups made: 0\n",
      "[[3, 0]]\n",
      "Ep 43 number of soups made: 0\n",
      "[[0, 3], [5, 0], [3, 0]]\n",
      "Ep 44 number of soups made: 0\n",
      "[]\n",
      "Ep 45 number of soups made: 0\n",
      "[[3, 0], [0, 3], [3, 0]]\n",
      "Ep 46 number of soups made: 0\n",
      "[[3, 0], [0, 3], [0, 5], [3, 0]]\n",
      "Ep 47 number of soups made: 0\n",
      "[[0, 3], [3, 0], [5, 0], [0, 3]]\n",
      "Ep 48 number of soups made: 0\n",
      "[[3, 0]]\n",
      "Ep 49 number of soups made: 0\n",
      "[[0, 3], [0, 5], [3, 0], [3, 0]]\n",
      "Ep 50 number of soups made: 0\n",
      "[[0, 3], [0, 5], [3, 0]]\n",
      "Ep 51 number of soups made: 0\n",
      "[[3, 0], [3, 0], [0, 3], [5, 0]]\n",
      "Ep 52 number of soups made: 0\n",
      "[[3, 0], [0, 3], [3, 0]]\n",
      "Ep 53 number of soups made: 0\n",
      "[[0, 3], [3, 0]]\n",
      "Ep 54 number of soups made: 0\n",
      "[[0, 3], [0, 5], [0, 3]]\n",
      "Ep 55 number of soups made: 0\n",
      "[[3, 0]]\n",
      "Ep 56 number of soups made: 0\n",
      "[[3, 0], [3, 0]]\n",
      "Ep 57 number of soups made: 0\n",
      "[[3, 0], [0, 5]]\n",
      "Ep 58 number of soups made: 0\n",
      "[[3, 0]]\n",
      "Ep 59 number of soups made: 0\n",
      "[[3, 0], [0, 3]]\n",
      "Ep 60 number of soups made: 0\n",
      "[[0, 3], [0, 3], [0, 3]]\n",
      "Ep 61 number of soups made: 0\n",
      "[]\n",
      "Ep 62 number of soups made: 0\n",
      "[[3, 0], [0, 3]]\n",
      "Ep 63 number of soups made: 0\n",
      "[[0, 3], [3, 0]]\n",
      "Ep 64 number of soups made: 0\n",
      "[[0, 3], [0, 3], [0, 5]]\n",
      "Ep 65 number of soups made: 0\n",
      "[[0, 3], [0, 3], [0, 3], [3, 0]]\n",
      "Ep 66 number of soups made: 0\n",
      "[[0, 3]]\n",
      "Ep 67 number of soups made: 0\n",
      "[[3, 0]]\n",
      "Ep 68 number of soups made: 0\n",
      "[[0, 3], [0, 3]]\n",
      "Ep 69 number of soups made: 0\n",
      "[[0, 3], [5, 0]]\n",
      "Ep 70 number of soups made: 0\n",
      "[[3, 0], [3, 0]]\n",
      "Ep 71 number of soups made: 0\n",
      "[[3, 0], [3, 0], [5, 0]]\n",
      "Ep 72 number of soups made: 0\n",
      "[[3, 0], [0, 3], [0, 5]]\n",
      "Ep 73 number of soups made: 0\n",
      "[[3, 0], [0, 3], [5, 0]]\n",
      "Ep 74 number of soups made: 0\n",
      "[[0, 3], [3, 0]]\n",
      "Ep 75 number of soups made: 0\n",
      "[[0, 3]]\n",
      "Ep 76 number of soups made: 0\n",
      "[[0, 3], [3, 0]]\n",
      "Ep 77 number of soups made: 0\n",
      "[[0, 3], [3, 0], [0, 5]]\n",
      "Ep 78 number of soups made: 0\n",
      "[[3, 0], [5, 0], [3, 0]]\n",
      "Ep 79 number of soups made: 0\n",
      "[[3, 0], [0, 3], [0, 5], [3, 0], [0, 3], [5, 0], [0, 3]]\n",
      "Ep 80 number of soups made: 0\n",
      "[[3, 0], [3, 0], [5, 0]]\n",
      "Ep 81 number of soups made: 0\n",
      "[[3, 0], [0, 3], [0, 5]]\n",
      "Ep 82 number of soups made: 0\n",
      "[[0, 3], [5, 0], [0, 3]]\n",
      "Ep 83 number of soups made: 0\n",
      "[[3, 0], [0, 5], [0, 3]]\n",
      "Ep 84 number of soups made: 0\n",
      "[[0, 3], [3, 0], [5, 0], [3, 0]]\n",
      "Ep 85 number of soups made: 0\n",
      "[[3, 0], [0, 5], [3, 0], [5, 0]]\n",
      "Ep 86 number of soups made: 0\n",
      "[[0, 3], [0, 3], [0, 3]]\n",
      "Ep 87 number of soups made: 0\n",
      "[[0, 3]]\n",
      "Ep 88 number of soups made: 0\n",
      "[[3, 0]]\n",
      "Ep 89 number of soups made: 0\n",
      "[[0, 3], [0, 3]]\n",
      "Ep 90 number of soups made: 0\n",
      "[[3, 0]]\n",
      "Ep 91 number of soups made: 0\n",
      "[[0, 3]]\n",
      "Ep 92 number of soups made: 0\n",
      "[[0, 3]]\n",
      "Ep 93 number of soups made: 0\n",
      "[[3, 0]]\n",
      "Ep 94 number of soups made: 0\n",
      "[[0, 3]]\n",
      "Ep 95 number of soups made: 0\n",
      "[[3, 0], [0, 3], [5, 0], [0, 3]]\n",
      "Ep 96 number of soups made: 0\n",
      "[[0, 3]]\n",
      "Ep 97 number of soups made: 0\n",
      "[[0, 3]]\n",
      "Ep 98 number of soups made: 0\n",
      "[[0, 3], [0, 3], [0, 3]]\n",
      "Ep 99 number of soups made: 0\n",
      "[]\n",
      "Ep 100 number of soups made: 0\n",
      "[[0, 3], [3, 0]]\n",
      "Ep 101 number of soups made: 0\n",
      "[[3, 0], [3, 0], [5, 0], [0, 3]]\n",
      "Ep 102 number of soups made: 0\n",
      "[[3, 0]]\n",
      "Ep 103 number of soups made: 0\n",
      "[[0, 3], [5, 0]]\n",
      "Ep 104 number of soups made: 0\n",
      "[[3, 0], [3, 0], [0, 3], [0, 3]]\n",
      "Ep 105 number of soups made: 0\n",
      "[[0, 3], [3, 0], [0, 5]]\n",
      "Ep 106 number of soups made: 0\n",
      "[[3, 0], [5, 0], [0, 3], [0, 5], [3, 0]]\n",
      "Ep 107 number of soups made: 0\n",
      "[[3, 0]]\n",
      "Ep 108 number of soups made: 0\n",
      "[[0, 3], [5, 0]]\n",
      "Ep 109 number of soups made: 0\n",
      "[[3, 0], [0, 3], [5, 0], [0, 3]]\n",
      "Ep 110 number of soups made: 0\n",
      "[[0, 3], [3, 0], [0, 3]]\n",
      "Ep 111 number of soups made: 0\n",
      "[[0, 3]]\n",
      "Ep 112 number of soups made: 0\n",
      "[[0, 3], [0, 3], [0, 5], [3, 0]]\n",
      "Ep 113 number of soups made: 0\n",
      "[[0, 3]]\n",
      "Ep 114 number of soups made: 0\n",
      "[[0, 3], [0, 3], [5, 0]]\n",
      "Ep 115 number of soups made: 0\n",
      "[[3, 0], [3, 0], [5, 0]]\n",
      "Ep 116 number of soups made: 0\n",
      "[[0, 3], [3, 0]]\n",
      "Ep 117 number of soups made: 0\n",
      "[[0, 3], [0, 3], [3, 0], [3, 0]]\n",
      "Ep 118 number of soups made: 0\n",
      "[[3, 0], [3, 0], [0, 3]]\n",
      "Ep 119 number of soups made: 0\n",
      "[[0, 3], [5, 0]]\n",
      "Ep 120 number of soups made: 0\n",
      "[[3, 0], [0, 3], [0, 5]]\n",
      "Ep 121 number of soups made: 0\n",
      "[[0, 3]]\n",
      "Ep 122 number of soups made: 0\n",
      "[[3, 0], [0, 3]]\n",
      "Ep 123 number of soups made: 0\n",
      "[[3, 0], [3, 0], [5, 0], [0, 3], [0, 3]]\n",
      "Ep 124 number of soups made: 0\n",
      "[[3, 0], [0, 3], [0, 3], [0, 5], [3, 0], [0, 3], [0, 5]]\n",
      "Ep 125 number of soups made: 0\n",
      "[]\n",
      "Ep 126 number of soups made: 0\n",
      "[[0, 3], [0, 3], [0, 5]]\n",
      "Ep 127 number of soups made: 0\n",
      "[[0, 3], [0, 3], [3, 0], [0, 3], [5, 0]]\n",
      "Ep 128 number of soups made: 0\n",
      "[[0, 3]]\n",
      "Ep 129 number of soups made: 0\n",
      "[[3, 0], [5, 0], [3, 0]]\n",
      "Ep 130 number of soups made: 0\n",
      "[[0, 3], [3, 0]]\n",
      "Ep 131 number of soups made: 0\n",
      "[[0, 3]]\n",
      "Ep 132 number of soups made: 0\n",
      "[[3, 0], [0, 3], [0, 3], [0, 3], [0, 5], [3, 0], [3, 0], [5, 0]]\n",
      "Ep 133 number of soups made: 0\n",
      "[[0, 3], [0, 3], [0, 5], [0, 3]]\n",
      "Ep 134 number of soups made: 0\n",
      "[[3, 0]]\n",
      "Ep 135 number of soups made: 0\n",
      "[]\n",
      "Ep 136 number of soups made: 0\n",
      "[[3, 0], [0, 3]]\n",
      "Ep 137 number of soups made: 0\n",
      "[]\n",
      "Ep 138 number of soups made: 0\n",
      "[]\n",
      "Ep 139 number of soups made: 0\n"
     ]
    }
   ],
   "source": [
    "### Train your agent ###\n",
    "\n",
    "# The code below runs a few episodes with a random agent.  Your learning algorithm\n",
    "# would go here.\n",
    "n_soups = []\n",
    "num_episodes = 500\n",
    "\n",
    "for e in range(num_episodes):\n",
    "    # Episode termination flag\n",
    "    done = False\n",
    "\n",
    "    # The number of soups the agent pair made during the episode\n",
    "    num_soups_made = 0\n",
    "    infos = []\n",
    "    # Reset the environment at the start of each episode\n",
    "    obs = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        # Obtain observations for each agent\n",
    "        obs0 = obs[\"both_agent_obs\"][0]\n",
    "        obs1 = obs[\"both_agent_obs\"][1]\n",
    "        \n",
    "        a0 = agent0.epsilon_greedy(obs0)\n",
    "        a1 = agent1.epsilon_greedy(obs1)\n",
    "\n",
    "        # Take the selected actions and receive feedback from the environment\n",
    "        # The returned reward \"R\" only reflects completed soups.  You can find\n",
    "        # the separate shaping rewards in the \"info\" variables\n",
    "        # info[\"shaped_r_by_agent\"][0] and info[\"shaped_r_by_agent\"][1].  Note that\n",
    "        # this shaping reward does *not* include the +20 reward for completed\n",
    "        # soups returned in \"R\".\n",
    "        s, R, done, info = env.step([a0, a1])\n",
    "\n",
    "        next_obs0 = s[\"both_agent_obs\"][0]\n",
    "        next_obs1 = s[\"both_agent_obs\"][1]\n",
    "\n",
    "        if info['policy_agent_idx'] == 0:\n",
    "            reward0 = info['shaped_r_by_agent'][0] + info['sparse_r_by_agent'][0]\n",
    "            reward1 = info['shaped_r_by_agent'][1] + info['sparse_r_by_agent'][1]\n",
    "        else:\n",
    "            reward1 = info['shaped_r_by_agent'][0] + info['sparse_r_by_agent'][0]\n",
    "            reward0 = info['shaped_r_by_agent'][1] + info['sparse_r_by_agent'][1]\n",
    "\n",
    "        if reward0 != 0 or reward1 != 0:\n",
    "            infos.append([reward0, reward1])\n",
    "\n",
    "        agent0.memorize(obs0, a0, reward0, next_obs0, done)\n",
    "        agent1.memorize(obs1, a1, reward1, next_obs1, done)\n",
    "\n",
    "        agent0.replay()\n",
    "        agent1.replay()\n",
    "        \n",
    "        # Accumulate the number of soups made\n",
    "        num_soups_made += int(R / 20) # Each served soup generates 20 reward\n",
    "\n",
    "        if done:\n",
    "            n_soups.append(num_soups_made)\n",
    "            #plot_soup()\n",
    "\n",
    "#print('Complete')\n",
    "#plot_soup(show_result=True)\n",
    "#plt.ioff()\n",
    "#plt.show()\n",
    "    # Display status\n",
    "    print(infos)\n",
    "    print(\"Ep {0}\".format(e + 1), end=\" \")\n",
    "    print(\"number of soups made: {0}\".format(num_soups_made))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluate your agent ###\n",
    "\n",
    "# This is where you would rollout episodes with your trained agent.\n",
    "# The below code is a partcular way to rollout episodes in a format\n",
    "# compatible with a state visualizer, if you'd like to visualize what your\n",
    "# agents are doing during episodes.  Visualization is in the next cell.\n",
    "\n",
    "class StudentPolicy(NNPolicy):\n",
    "    \"\"\" Generate policy \"\"\"\n",
    "    def __init__(self):\n",
    "        super(StudentPolicy, self).__init__()\n",
    "\n",
    "    def state_policy(self, state, agent_index):\n",
    "        \"\"\"\n",
    "        This method should be used to generate the poiicy vector corresponding to\n",
    "        the state and agent_index provided as input.  If you're using a neural\n",
    "        network-based solution, the specifics depend on the algorithm you are using.\n",
    "        Below are two commented examples, the first for a policy gradient algorithm\n",
    "        and the second for a value-based algorithm.  In policy gradient algorithms,\n",
    "        the neural networks output a policy directly.  In value-based algorithms,\n",
    "        the policy must be derived from the Q value outputs of the networks.  The\n",
    "        uncommented code below is a placeholder that generates a random policy.\n",
    "        \"\"\"\n",
    "        featurized_state = base_env.featurize_state_mdp(state)\n",
    "        input_state = torch.FloatTensor(featurized_state[agent_index]).unsqueeze(0)\n",
    "\n",
    "        # Example for policy NNs named \"PNN0\" and \"PNN1\"\n",
    "        # with torch.no_grad():\n",
    "        #   if agent_index == 0:\n",
    "        #       action_probs = PNN0(input_state)[0].numpy()\n",
    "        #   else:\n",
    "        #       action_probs = PNN1(input_state)[0].numpy()\n",
    "\n",
    "        # Example for Q value NNs named \"QNN0\" and \"QNN1\"\n",
    "        # action_probs = np.zeros(env.action_space.n)\n",
    "        # with torch.no_grad():\n",
    "        #   if agent_index == 0:\n",
    "        #       action_probs[np.argmax(QNN0(input_state)[0].numpy())] = 1\n",
    "        #   else:\n",
    "        #       action_probs[np.argmax(QNN1(input_state)[0].numpy())] = 1\n",
    "\n",
    "        # Random deterministic policy\n",
    "        action_probs = np.zeros(env.action_space.n)\n",
    "        action_probs[env.action_space.sample()] = 1\n",
    "\n",
    "        return action_probs\n",
    "\n",
    "    def multi_state_policy(self, states, agent_indices):\n",
    "        \"\"\" Generate a policy for a list of states and agent indices \"\"\"\n",
    "        return [self.state_policy(state, agent_index) for state, agent_index in zip(states, agent_indices)]\n",
    "\n",
    "\n",
    "class StudentAgent(AgentFromPolicy):\n",
    "    \"\"\"Create an agent using the policy created by the class above\"\"\"\n",
    "    def __init__(self, policy):\n",
    "        super(StudentAgent, self).__init__(policy)\n",
    "\n",
    "\n",
    "# Instantiate the policies for both agents\n",
    "policy0 = StudentPolicy()\n",
    "policy1 = StudentPolicy()\n",
    "\n",
    "# Instantiate both agents\n",
    "agent0 = StudentAgent(policy0)\n",
    "agent1 = StudentAgent(policy1)\n",
    "agent_pair = AgentPair(agent0, agent1)\n",
    "\n",
    "# Generate an episode\n",
    "ae = AgentEvaluator.from_layout_name({\"layout_name\": layout}, {\"horizon\": horizon})\n",
    "trajs = ae.evaluate_agent_pair(agent_pair, num_games=1)\n",
    "print(\"\\nlen(trajs):\", len(trajs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Agent Visualization ###\n",
    "\n",
    "##############################################################################\n",
    "# The function StateVisualizer() below generates images for the state of the\n",
    "# environment at each time step of the episode.\n",
    "#\n",
    "# You have several options for how to use these images:\n",
    "#\n",
    "# 1) You can set img_dir to a local directory (or a directory within Google Drive\n",
    "# if using Colab), and all the images will be saved to that directory for you to browse.\n",
    "#\n",
    "# 2) If using a notebook, you can set the argument ipthon_display=True to get a\n",
    "# tool with a slider that lets you scan through all the images directly in the\n",
    "# notebook.  This option does not require you to store your images.\n",
    "#\n",
    "# 3) You can generate a GIF of the episode. This requires you to set\n",
    "# img_dir.  The code to generate the GIF is commented out below\n",
    "\n",
    "# Modify as appropriate\n",
    "img_dir = None\n",
    "ipython_display = True\n",
    "gif_path = None\n",
    "\n",
    "# Do not modify -- uncomment for GIF generation\n",
    "StateVisualizer().display_rendered_trajectory(trajs, img_directory_path=img_dir, ipython_display=ipython_display)\n",
    "# img_list = [f for f in os.listdir(img_dir) if f.endswith('.png')]\n",
    "# img_list.sort(key=lambda x: os.path.getmtime(os.path.join(img_dir, x)))\n",
    "# images = [Image.open(img_dir + img).convert('RGBA') for img in img_list]\n",
    "# images[0].save(gif_path, save_all=True, append_images=images[1:], optimize=False, duration=250, loop=0)\n",
    "# with open(gif_path,'rb') as f: display(IPImage(data=f.read(), format='png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPvD5jSPp+kwOFfA/X3s/LE",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
